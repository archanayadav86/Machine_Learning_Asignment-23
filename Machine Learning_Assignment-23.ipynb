{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the key reasons for reducing the dimensionality of a dataset? What are the major disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Dimensionality reduction brings many advantages to machine learning data, including: Fewer features mean less complexity. Will need less storage space because we have fewer data. Fewer features require less computation time.\n",
    "\n",
    "Disadvantages of PCA:\n",
    "Low interpretability of principal components. Principal components are linear combinations of the features from the original data, but they are not as easy to interpret. \n",
    "The trade-off between information loss and dimensionality reduction.\n",
    "Principal Components are not as readable and interpretable as original features. \n",
    "Data standardization is must before PCA: We must standardize your data before implementing PCA, otherwise PCA will not be able to find the optimal Principal Components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the dimensionality curse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how can you go about doing it? If not, what is the reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> No, dimensionality reduction is not reversible in general. It losses information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> PCA can be used to significantly reduce the dimensionality of most datasets, even if they are highly nonlinear because it can at least get rid of useless dimensions. However, if there are no useless dimensions, reducing dimensionality with PCA will lose too much information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Assume you're running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio. What is the number of dimensions that the resulting dataset would have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Will try to minimize the dimension only upto the extend where information loss is minimum once the information loss starts increasing will stop reducing the dimension as we may get less acuuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> Vanilla PCA: the dataset fit in memory\n",
    "Incremental PCA: larget dataset that don't fit in memory, online taks\n",
    "Randomized PCA: considerably reduce dimensionality and the dataset fit the memory.\n",
    "kenrl PCA: used for nonlinear PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. How do you assess a dimensionality reduction algorithm's success on your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans--> \n",
    "A dimensionality reduction algorithm is said to work well if it eliminates a significant number of dimensions from the dataset without losing too much information.\n",
    "\n",
    "We can assess by below methods-\n",
    "\n",
    "measure the reconstruction error\n",
    "measure the performance in second Machine Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Is it logical to use two different dimensionality reduction algorithms in a chain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans--> YES, It can absolutely make sense to chain two different dimensionality reduction algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
